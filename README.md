# yeest.xyz - RAG-Powered Chat Assistant

A modern, text-only RAG (Retrieval-Augmented Generation) powered chat assistant that provides real-time information from Wikipedia, news sources, and Reddit discussions. Built with LangChain, GROQ LLM, and comprehensive professional features including memory management, cost monitoring, and automated evaluation.

## ğŸš€ Features

- **Real-time Information Retrieval**: Fetches fresh content from Wikipedia, news APIs, and Reddit
- **Advanced Memory Management**: Session buffer + conversation summarization
- **Professional Monitoring**: LangSmith integration for cost tracking and performance monitoring
- **Automated Evaluation**: ROUGE-based evaluation pipeline with human-in-the-loop capabilities
- **Modern Tech Stack**: FastAPI backend, Next.js frontend, Docker deployment ready
- **Extensible Architecture**: Easy to add new data sources and customize

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚    Backend      â”‚    â”‚  Data Sources   â”‚
â”‚   (Next.js)     â”‚â—„â”€â”€â–ºâ”‚   (FastAPI)     â”‚â—„â”€â”€â–ºâ”‚  Wikipedia      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚  News API       â”‚
â”‚   - Chat UI     â”‚    â”‚   - RAG Chain   â”‚    â”‚  Reddit         â”‚
â”‚   - TailwindCSS â”‚    â”‚   - Vector DB   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   - TypeScript  â”‚    â”‚   - Memory Mgmt â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   - LangSmith   â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   Monitoring    â”‚
                                              â”‚   LangSmith     â”‚
                                              â”‚   Evaluation    â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tech Stack

### Backend
- **Python 3.11+** with FastAPI
- **LangChain** for RAG implementation
- **GROQ** for LLM inference (free tier)
- **Chroma** for vector storage
- **HuggingFace** embeddings
- **LangSmith** for monitoring and tracing

### Frontend
- **Next.js 14** with TypeScript
- **React 18** for UI components
- **TailwindCSS** for styling
- **Axios** for API communication

### Data Sources
- **Wikipedia** via wikipedia-python
- **News API** for current events
- **Reddit** via PRAW

## ğŸš€ Quick Start

### Prerequisites

1. **GROQ API Key** (required) - Get from [console.groq.com](https://console.groq.com)
2. **News API Key** (optional) - Get from [newsapi.org](https://newsapi.org)
3. **Reddit OAuth** (optional) - Get from [reddit.com/prefs/apps](https://reddit.com/prefs/apps)
4. **LangSmith API Key** (optional) - Get from [smith.langchain.com](https://smith.langchain.com)

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd yeest
   ```

2. **Backend Setup**
   ```bash
   cd backend
   cp .env.example .env
   # Edit .env with your API keys
   pip install -r requirements.txt
   ```

3. **Frontend Setup**
   ```bash
   cd frontend
   npm install
   ```

### Running the Application

#### Option 1: Docker Compose (Recommended)
```bash
# Copy environment variables
cp backend/.env.example backend/.env
# Edit backend/.env with your API keys

# Start both services
docker-compose up --build
```

#### Option 2: Manual Setup
```bash
# Terminal 1 - Backend
cd backend
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Terminal 2 - Frontend
cd frontend
npm run dev
```

### Access the Application
- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs

## ğŸ“ Configuration

### Environment Variables

Create `backend/.env` with the following variables:

```env
# Required
GROQ_API_KEY=your_groq_api_key_here

# Optional - for enhanced functionality
NEWSAPI_KEY=your_newsapi_key_here
REDDIT_CLIENT_ID=your_reddit_client_id_here
REDDIT_CLIENT_SECRET=your_reddit_client_secret_here
LANGSMITH_API_KEY=your_langsmith_api_key_here

# Configuration (optional)
GROQ_MODEL_NAME=mixtral-8x7b-32768
VECTOR_STORE_PATH=./chroma_db
CHUNK_SIZE=500
CHUNK_OVERLAP=50
MEMORY_BUFFER_SIZE=8
RAG_K=5
```

## ğŸ§ª Testing

### Backend Tests
```bash
cd backend
pytest tests/ -v
```

### Frontend Tests
```bash
cd frontend
npm run lint
npm run type-check
npm run build
```

### Evaluation Pipeline
```bash
cd backend
python -m app.eval_runner
```

## ğŸ“Š Monitoring & Evaluation

### LangSmith Integration
- Automatic tracing of all LLM calls
- Cost monitoring and token usage tracking
- Performance metrics and latency analysis
- Access dashboard at [smith.langchain.com](https://smith.langchain.com)

### Automated Evaluation
- ROUGE-based scoring against reference answers
- Continuous evaluation in CI/CD pipeline
- Human-in-the-loop annotation support
- Results logged to LangSmith for analysis

## ğŸš€ Deployment

### Docker Deployment
```bash
# Build and run with Docker Compose
docker-compose up --build -d

# Or build individual containers
docker build -t yeest-backend ./backend
docker build -t yeest-frontend ./frontend
```

### Cloud Deployment Options

#### Backend
- **AWS ECS** / **Google Cloud Run** / **Fly.io**
- **Heroku** / **Railway** / **Render**

#### Frontend
- **Vercel** (recommended for Next.js)
- **Netlify** / **AWS Amplify**

### Environment Setup for Production
1. Set up secret management (AWS Secrets Manager, etc.)
2. Configure CORS origins for production domains
3. Set up monitoring and logging
4. Configure auto-scaling based on usage

## ğŸ”§ Development

### Project Structure
```
yeest/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”‚   â”œâ”€â”€ llm.py               # LLM factory
â”‚   â”‚   â”œâ”€â”€ rag.py               # RAG implementation
â”‚   â”‚   â”œâ”€â”€ memory.py            # Memory management
â”‚   â”‚   â”œâ”€â”€ utils.py             # Utility functions
â”‚   â”‚   â”œâ”€â”€ eval_runner.py       # Evaluation pipeline
â”‚   â”‚   â””â”€â”€ retrievers/          # Data source retrievers
â”‚   â”œâ”€â”€ tests/                   # Backend tests
â”‚   â”œâ”€â”€ requirements.txt         # Python dependencies
â”‚   â””â”€â”€ Dockerfile              # Backend container
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ pages/                   # Next.js pages
â”‚   â”œâ”€â”€ components/              # React components
â”‚   â”œâ”€â”€ styles/                  # CSS styles
â”‚   â”œâ”€â”€ package.json            # Node.js dependencies
â”‚   â””â”€â”€ Dockerfile              # Frontend container
â”œâ”€â”€ .github/workflows/           # CI/CD pipelines
â””â”€â”€ docker-compose.yml          # Development setup
```

### Adding New Data Sources
1. Create a new retriever in `backend/app/retrievers/`
2. Implement the retrieval function returning LangChain Documents
3. Add the retriever to `backend/app/rag.py`
4. Update the frontend to display source metadata

### Customizing the LLM
1. Modify `backend/app/llm.py` to use different models
2. Update prompts in `backend/app/rag.py`
3. Adjust memory settings in `backend/app/config.py`

## ğŸ“ˆ Performance Optimization

### Backend
- **Caching**: Implement Redis for popular queries
- **Vector Store**: Consider Pinecone for production scale
- **Async Processing**: Use background tasks for heavy operations
- **Rate Limiting**: Implement API rate limiting

### Frontend
- **Code Splitting**: Automatic with Next.js
- **Image Optimization**: Use Next.js Image component
- **Caching**: Implement SWR or React Query
- **Performance Monitoring**: Add analytics and monitoring

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **LangChain** for the RAG framework
- **GROQ** for fast LLM inference
- **Anthropic** for Claude model inspiration
- **Vercel** for Next.js framework
- **FastAPI** for the backend framework

## ğŸ“ Support

For support, please open an issue on GitHub or contact the development team.

---

**Built with â¤ï¸ for the AI community**
